# -*- coding: utf-8 -*-
"""idx_credit_risk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rsT2BQRPEDQ2uFftp6hEGVOT3MxKcrcs

### **1. Data Understanding**

#### **a. Import Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""#### **b. Load dataset**"""

# Load dataset
df = pd.read_csv('loan_data_2007_2014.csv')

# Tampilkan 5 baris pertama dari dataframe
df.head()

"""#### **c. Identified Dataset Structure**"""

df.shape

"""Data terdiri dari 75 kolom dan 466285 baris"""

# Cek info awal data
df.info()

"""#### **d. Handle Missing Value**"""

# Cek kolom yang hanya berisi bais kosong
empty_columns = df.columns[df.isna().all()].tolist()

print("Kolom yang seluruh barisnya kosong:")
print(empty_columns)

# Hapus kolom kosong
df.dropna(axis=1, how='all', inplace=True)
df.shape

"""Setelah menghapus kolom yang kosong kolom pada data berkurang menjadi 58"""

# Cek kolom kosong > 50%
cols_over_50_missing = df.columns[df.count() < df.shape[0] * 0.5].tolist()
print("Kolom dengan lebih dari 50% nilai kosong:")
print(cols_over_50_missing)

# Hapus kolom dengan 50% lebih baris kosong
df.dropna(thresh = df.shape[0]*0.5 , axis=1 , inplace=True)
df.shape

"""Kini data hanya memiliki 54 kolom

##### **e. Create a New Column Credit Status Label**
"""

df['loan_status'].value_counts()

df['status'] = np.where(df.loc[:, 'loan_status'].isin(['Charged Off',
                                                        'Default',
                                                        'Late (31-120 days)',
                                                        'Late (16-30 days)',
                                                        'Does not meet the credit policy. Status:Charged Off'
                                                        ]),
                            1, 0)

"""Jika pada kolom loan_status terdapat 'Charged Off, 'default,
'Late (31-120 days)', 'Late (16-30 days)', atau
'Does not meet the credit policy. Status:Charged Off' maka status credit BAD(1) jika tidak ada maka status credit GOOD(0)
"""

df['status'].value_counts()

"""#### **2. EDA**

#### **a. Descriptive Statistics**
"""

df.info()

# Drop kolo yang tidak relevan dengan Status Kredit
df.drop(columns=['Unnamed: 0', 'id', 'member_id', 'policy_code', 'emp_title', 'url', 'title',
                 'zip_code', 'addr_state', 'loan_status', 'application_type'],inplace=True)

df.describe()

# Mengelompokkan kolom yang seharusnya memilik tipe data datetime
date_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d',
            'last_credit_pull_d']
print("Kolom tanggal:", date_cols)

# Merubah col_date menjadi tipe data 'datetime'
for col in date_cols :
    df[col] = pd.to_datetime(df[col], format='%b-%y')

df[date_cols].info()

# Membagi data menjadi kategorikal dan numerikal
categorical_cols = df.select_dtypes(include=['object', 'category']).columns
numerical_cols = df.select_dtypes(include=['number']).columns

print("Kolom kategorikal:", categorical_cols)
print("Kolom numerikal:", numerical_cols)

"""##### **b. Handling Missing Value**"""

# Mengisi nilai yang hilang pada kolom kategorikal dengan nilai modusnya
for col in categorical_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

df[categorical_cols].info()

# Mengisi nilai yang hilang pada kolom numerik dengan nilai mediannya
for col in numerical_cols:
    df[col] = df[col].fillna(df[col].median())

df[categorical_cols].info()

# Mengisi nilai yang hilang pada kolom datetime dengan nilai modusnya
for col in date_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

df[categorical_cols].info()

df.info()

"""Terlihat tidak ada lagi nilai yang hilang pada setiap kolom

#### **c. Univariate Analysis**

##### **Categorical column**
"""

num_var_cat = len(categorical_cols)
n_cols = 4
n_rows =  -(-num_var_cat // n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
axes = axes.flatten()

for i, col in enumerate(categorical_cols):
    sns.countplot(x=col, data=df, ax=axes[i])
    axes[i].set_title(f'Distribusi {col}')
    axes[i].tick_params(axis='x', rotation=90)

for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""- Kolom 'term' didominasi dengan jangka waktu peminjaman 36 bulan dibanding dengan jangka waktu peminjaan 60 bulan.
- Kolom 'grade' mayoritas diisi dengan grade B dan C dengan nilai yang hampir sama.
- Kolom 'emp_length' lebih banyak peminjam yang sudah bekerja lebih dari 10 tahun.
- Kolom 'home_ownership' mayoritas berisi 'mortage'
- Kolom 'verification_status' berisi distribusi yang hampir seimbang dengan verified memiliki jumlah paling banyak
- Kolom 'payment_plant' hampir semuanya berisi label n
- Kolom 'purpose' didominasi oleh debt consolidation
- Kolom 'initial_list_status' mayoritas berisi label f

##### **Numerical column**
"""

num_var_num = len(numerical_cols)
n_cols = 4
n_rows = -(-num_var_num // n_cols)

plt.figure(figsize=(15, n_rows * 4))

for i in range(num_var_num):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.boxplot(x=df[numerical_cols[i]])
    plt.xlabel(numerical_cols[i])

plt.tight_layout()
plt.show()

"""- Dataset cenderung memiliki banyak outlier pada bagian kanan (skewed kanan) yang nantinya perlu ditangani.

##### **Date column**
"""

num_var_date = len(df[date_cols])
n_cols = 2
n_rows =  -(-num_var_cat // n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(8*n_cols, 4*n_rows))
axes = axes.flatten()

for i, col in enumerate(date_cols):
    df[col].dt.year.value_counts().sort_index().plot(kind='bar', ax=axes[i])
    axes[i].set_title(f'Distribusi Waktu {col}')
    axes[i].set_xlabel('Tahun')
    axes[i].set_ylabel('Jumlah')
    axes[i].tick_params(axis='x', rotation=90)

for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""- Setiap tahunnya cenderung mengalami kenaikan pada kolom datetime

#### **d. Bivariate analysis**

##### **Categorical column**
"""

n_cols = 4
n_rows =  -(-num_var_cat // n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
axes = axes.flatten()

for i, col in enumerate(categorical_cols):
    sns.countplot(x=col, hue="status", data=df, ax=axes[i])
    axes[i].set_title(f'{col} vs status')
    axes[i].tick_params(axis='x', rotation=45)

for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""- Status kredit yang GOOD didominasi dengan tenor peminjaman 36 bulan, grade peminjaman B atau C, mayoritas telah bekerja lebih dari 10 tahun, kepemilikan rumah masih kredit, dan tujuan peminjaman untuk debt consolidation.
- Sedangkan status kredit yang BAD kebanyakan meminjam dengan tenor 36 bulan, semakin jelek gradenya presentase statusnya semakin besar, telah bekerja lebih dari 10 tahun, kepemilikan rumah juga masih kredit, dan tujuan peminjaman untuk debt consolidation.

##### **Numerical column**
"""

n_cols = 4
n_rows =  -(-num_var_num // n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(15 , 4*n_rows))
axes = axes.flatten()

for i, col in enumerate(numerical_cols):
    sns.boxplot(x='status', y=col, data=df, ax=axes[i])
    axes[i].set_title(f'{col} vs status')

for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""- Status kredit BAD cenderung memiliki annual income, total payment yang cenderung rendah. Sedangkan biaya keterlambatannya tinggi

##### **Date column**
"""

plt.figure(figsize=(15, 8))
for i, col in enumerate(date_cols):
    plt.subplot(3, 2, i + 1)
    df_temp = df.copy()
    df_temp[col + '_year'] = df_temp[col].dt.year

    sns.countplot(x=col + '_year', hue='status', data=df_temp, palette='viridis')
    plt.xlabel(col + ' Year')
    plt.ylabel('Count')
    plt.title(f'Distribution of {col} Year by Status')
    plt.xticks(rotation=90)
    plt.tight_layout()

plt.show()

"""Hampir setiap tahunnya terjadi peningkatan status GOOD dan BAD dengan mayoritas berisi kelas GOOD

##### **e. Multivariate Analysis**
"""

# Hitung matriks korelasi
correlation_matrix = df[numerical_cols].corr()

# Buat Heatmap
plt.figure(figsize=(20, 20))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap="Blues")
plt.show()

"""Berdasarkan correlation matrix terdapat beberapa fitur yang berkolerasi sangat tinggi di atas 0.9. Salah satu dari fitur yang berkolerasi tinggi ini akan dihapus nantinya untuk mengurarangi redundancy data.
"""

"""## **3. Data Preparation**

#### **a. Feature Engineering**
"""

# Mengambil fitur yang berkolerasi di atas 0.9
high_corr = correlation_matrix.where((correlation_matrix.abs() > 0.9) & (correlation_matrix.abs() < 1.0))

# Ubah menjadi stack dan drop NaN
high_corr_pairs = high_corr.stack().reset_index()
high_corr_pairs.columns = ['Feature1', 'Feature2', 'Correlation']

print(high_corr_pairs)

# Menghapus kolom yang tidak digunakan pada pembuatan model
# Menghapus kolom yang memiliki korelasi sangat tinggi (salah satunya saja)
df.drop(columns=['collections_12_mths_ex_med', 'acc_now_delinq', 'tot_coll_amt', 'pymnt_plan', 'funded_amnt_inv',
                 'installment', 'out_prncp_inv', 'total_pymnt_inv', 'total_rec_prncp'],inplace=True)

# Melakukan mapping untuk data-data ordinal
mappings = {
    'emp_length': {
        "< 1 year": 0, "1 year": 1, "2 years": 2, "3 years": 3, "4 years": 4,
        "5 years": 5, "6 years": 6, "7 years": 7, "8 years": 8, "9 years": 9, "10+ years": 10
    },
    'grade': {
        "A": 0, "B": 1, "C": 2, "D": 3, "E": 4, "F": 5, "G": 6
    },
    'verification_status': {
        "Not Verified": 0, "Source Verified": 1, "Verified": 2
    }
}

for col, mapping in mappings.items():
    df[col] = df[col].map(mapping).astype(int)

df[['emp_length', 'grade', 'verification_status']]

"""##### **b. Outlier Handling**"""

# Membagi kolom menjadi fitur kategorikal dan numerikal yang nantinya akan dilatih nantinya
categorical_features = df.select_dtypes(include=['object', 'category']).columns
numerical_features = df.select_dtypes(include=['number']).drop(columns=['status']).columns

numerical_features

# Menangani outliers untuk setiap kolom numerik dengan mengganti outliers dengan median
for column in numerical_features:
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    median = df[column].median()

    # Mengganti nilai outlier dengan median
    df[column] = df[column].apply(
        lambda x: median if x < (Q1 - 1.5 * IQR) or x > (Q3 + 1.5 * IQR) else x
    )

print("Data setelah menangani outliers:")
df.describe()

"""#### **c. Encoding Data**"""

from sklearn.preprocessing import LabelEncoder

# Melakukan encoding untuk mengubah data kategorikal menjadi numerikal
label_encoder = LabelEncoder()
df_encoding = df.copy()

for col in categorical_features:
    df_encoding[col] = label_encoder.fit_transform(df[col])

df_encoding

"""#### **e. Scalling Data**"""

from sklearn.preprocessing import StandardScaler

# Standardisasi fitur numerik
scaler = StandardScaler()
df_normalized = df_encoding.copy()
df_normalized[numerical_features] = scaler.fit_transform(df_encoding[numerical_features])

print(df_normalized)

"""##### **f. Splitting Data**"""

# Melihat distribusi kelas target 'status'
pallete = sns.color_palette('deep')

plt.figure(figsize=(6,6))
plt.pie(df['status'].value_counts(),
        labels=['GOOD', 'BAD'],
        autopct='%1.1f%%',
        startangle=90,
        colors=pallete)
plt.title('Komposisi Credit Risk')

plt.show()

"""Dapat dilihat di atas kelas target yaitu 'status' antara GOOD dan BAD tidak seimbang perbandingannya maka dari itu akan dilakukan Handling Imbalance Data pada data training menggunakan SMOTE dengan cara melakukna oversampling pada kelas minority."""

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from collections import Counter

X = df_normalized.drop('status', axis=1)

X = X.select_dtypes(exclude=['datetime64'])
y = df_normalized['status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("Distribusi sebelum SMOTE:", Counter(y_train))

smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print("Distribusi setelah SMOTE:", Counter(y_train_resampled))

"""## **5. Data Modelling**"""

# Import dan definisikan model yang akan digunakan
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier

lr = LogisticRegression()
dt = DecisionTreeClassifier()
rf = RandomForestClassifier()
xg = XGBClassifier()

"""#### **a. Logistic Regression**"""

lr.fit(X_train_resampled, y_train_resampled)
y_pred_lr = lr.predict(X_test)

"""#### **b. Decision Tree**"""

dt.fit(X_train_resampled, y_train_resampled)
y_pred_dt = dt.predict(X_test)

"""#### **c. Random Forest**"""

rf.fit(X_train_resampled, y_train_resampled)
y_pred_rf= rf.predict(X_test)

"""#### **d. XGBoost**"""

xg.fit(X_train_resampled, y_train_resampled)
y_pred_xg= xg.predict(X_test)

"""#### **e. Accuration Score**"""

from sklearn.metrics import accuracy_score

print("Akurasi Logistic Regression :", accuracy_score(y_test, y_pred_lr))
print("Akurasi Decision Tree :", accuracy_score(y_test, y_pred_dt))
print("Akurasi Random Forest :", accuracy_score(y_test, y_pred_rf))
print("Akurasi XGBoost :", accuracy_score(y_test, y_pred_xg))

"""#### **GridSearch XGBoost**

Berdasarkan 4 model yang dilatih di atas XGBoost memiliki akurasi paling tinggi. Maka dilakukan GridSearch dari model tersebut sebagai usaha untuk mencari hyperparameter terbaik dari model tersebut.
"""

from sklearn.model_selection import train_test_split, GridSearchCV

# Parameter Grid
param_grid = {
    'n_estimators': [100],
    'max_depth': [4, 6],
    'learning_rate': [0.1, 0.2]
}

grid_search = GridSearchCV(
    estimator=xg,
    param_grid=param_grid,
    cv=3,
    scoring='accuracy',
    verbose=2,
    n_jobs=-3
)

grid_search.fit(X_train_resampled, y_train_resampled)

# Parameter terbaik GridSearch
print("Parameter terbaik:", grid_search.best_params_)
print("Skor Terbaik:", grid_search.best_score_)

best_model_xg = grid_search.best_estimator_
y_pred_gs_xg = best_model_xg.predict(X_test)
print("Akurasi pada data test:", accuracy_score(y_test, y_pred_gs_xg))

"""Sejauh hasil GridSearch yang dilakukan, parameter terbaik dari model XGboost untuk dataset ini yaitu dengan 'learning_rate': 0.2, 'max_depth': 6, dan'n_estimators': 100.

## **4. Evaluation**

#### **a. Confusion Matrix**
"""

from sklearn.metrics import confusion_matrix

# Daftar prediksi dan label model
model_preds = [
    (y_pred_lr, "Logistic Regression"),
    (y_pred_dt, "Decision Tree"),
    (y_pred_rf, "Random Forest"),
    (y_pred_xg, "XGBoost"),
    (y_pred_gs_xg, "XGBoost GridSearch")
]

fig, axes = plt.subplots(3, 2, figsize=(12, 10))
axes = axes.flatten()

for i, (y_pred, title) in enumerate(model_preds):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='.0f', cmap=plt.cm.Blues, ax=axes[i])
    axes[i].set_title(f"Confusion Matrix - {title}")
    axes[i].set_xlabel("Predicted")
    axes[i].set_ylabel("Actual")

if len(model_preds) < len(axes):
    axes[-1].axis('off')


plt.tight_layout()
plt.subplots_adjust(wspace=0.3)
plt.show()

"""#### **b. Evaluation Metrics**"""

from sklearn.metrics import classification_report

print("Evaluation metrics Logsitic Regression")
print(classification_report(y_test, y_pred_lr))

print("\nEvaluation metrics Decision Tree")
print(classification_report(y_test, y_pred_dt))

print("\nEvaluation metrics Random Forest")
print(classification_report(y_test, y_pred_rf))

print("\nEvaluation metrics XGBoost")
print(classification_report(y_test, y_pred_xg))

print("\nEvaluation metrics XGBoost - Grid Search")
print(classification_report(y_test, y_pred_gs_xg))

"""#### **c. ROC Curve**"""

from sklearn.metrics import roc_curve, auc

def plot_roc_curve(model, X_test_roc, y_test_roc, model_name="Model", show=False):
    """
    Menampilkan ROC Curve dari setiap model.

    Parameters:
    - model: objek model yang sudah dilatih
    - X_test_roc: fitur data uji
    - y_test_roc: label asli data uji
    - model_name: nama model untuk label di plot
    """
    # Prediksi probabilitas kelas positif (kelas 1)
    y_prob = model.predict_proba(X_test_roc)[:, 1]

    # Hitung ROC dan AUC
    fpr, tpr, _ = roc_curve(y_test_roc, y_prob)
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve
    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')

    if show:
        plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve Comparison')
        plt.legend(loc="lower right")
        plt.grid()
        plt.show()

# Plot ROC Curve dari setiap model
plt.figure(figsize=(8, 6))

plot_roc_curve(lr, X_test, y_test, model_name="Logistic Regression")
plot_roc_curve(dt, X_test, y_test, model_name="Decision Treee")
plot_roc_curve(rf, X_test, y_test, model_name="Random Forest")
plot_roc_curve(xg, X_test, y_test, model_name="XGBoost")
plot_roc_curve(best_model_xg, X_test, y_test, model_name="XGBoost GridSearch", show=True)

"""# **Conclusion**
Dari keempat model klasifikasi yang diuji, yaitu Logistic Regression, Decision Tree, Random Forest, dan XGBoost, model XGBoost menunjukkan performa terbaik dibandingkan model lainnya. Hal ini terlihat dari hasil Confusion Matrix dan evaluasi metrik yang menunjukkan bahwa XGBoost memiliki tingkat kesalahan klasifikasi paling rendah serta nilai metrik yang paling tinggi. Meskipun penerapan GridSearch belum berhasil meningkatkan akurasi secara signifikan, masih terdapat peluang untuk mengoptimalkan performa model XGBoost dengan menambahkan atau menyesuaikan parameter lain dalam proses tuning. Oleh karena itu, model XGBoost merupakan pilihan yang paling direkomendasikan untuk melakukan klasifikasi risiko kredit.

# **Business Recomendation**

Menerapkan model XGBoost sebagai model utama dalam sistem klasifikasi risiko kredit karena memiliki akurasi dan F1-score tertinggi dibandingkan model lainnya. Keunggulan ini menjadikannya alat yang andal dalam mengidentifikasi nasabah berisiko tinggi secara lebih akurat.

Model XGBoost juga dapat diintegrasikan ke dalam sistem evaluasi pengajuan pinjaman untuk mempercepat proses pengambilan keputusan. Sebagai contoh, jika model memprediksi risiko kredit = "Bad", sistem dapat langsung memberikan notifikasi ke tim analis untuk verifikasi manual atau permintaan jaminan tambahan sebelum keputusan akhir dibuat.
"""